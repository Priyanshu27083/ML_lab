{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPC+yzk2Bl+PVq1/79dptxp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyanshu27083/ML_lab/blob/main/Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nr5wpwh1JoeR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7447d7b-ae19-4531-fe61-7aaf44f6c6cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9990889488055994\n"
          ]
        }
      ],
      "source": [
        "#Simple perceptron\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  # Our activation function: f(x) = 1 / (1 + e^(-x))\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class Neuron:\n",
        "  def __init__(self, weights, bias):\n",
        "    self.weights = weights\n",
        "    self.bias = bias\n",
        "\n",
        "  def feedforward(self, inputs):\n",
        "    # Weight inputs, add bias, then use the activation function\n",
        "    total = np.dot(self.weights, inputs) + self.bias\n",
        "    return sigmoid(total)\n",
        "\n",
        "weights = np.array([0, 1]) # w1 = 0, w2 = 1\n",
        "bias = 4                   # b = 4\n",
        "n = Neuron(weights, bias)\n",
        "\n",
        "x = np.array([2, 3])       # x1 = 2, x2 = 3\n",
        "print(n.feedforward(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "single neuron takes multiple inputs, multiplies them by their corresponding weights, and adds a bias. It then passes this weighted sum through a sigmoid activation function, which converts the value into a number between 0 and 1. Finally, the neuron is created with specific weights and bias, and its output is computed for a given input array, representing the neuronâ€™s activation level."
      ],
      "metadata": {
        "id": "tsp7_lRlik6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "   def __init__(self, learning_rate=0.1, epochs=100):\n",
        "      self.learning_rate = learning_rate\n",
        "      self.epochs = epochs\n",
        "      self.weights = None\n",
        "      self.bias = None\n",
        "\n",
        "   def step_function(self, x):\n",
        "      return np.where(x >= 0, 1, 0)\n",
        "   def fit(self, X, y):\n",
        "      n_samples, n_features = X.shape\n",
        "\n",
        "      # initialize weights and bias to 0\n",
        "      self.weights = np.zeros(n_features)\n",
        "      self.bias = 0\n",
        "\n",
        "      # iterate over epochs and update weights and bias\n",
        "      for _ in range(self.epochs):\n",
        "         for i in range(n_samples):\n",
        "            linear_output = np.dot(self.weights, X[i]) + self.bias\n",
        "            y_pred = self.step_function(linear_output)\n",
        "\n",
        "            # update weights and bias based on error\n",
        "            update = self.learning_rate * (y[i] - y_pred)\n",
        "            self.weights += update * X[i]\n",
        "            self.bias += update\n",
        "\n",
        "   def predict(self, X):\n",
        "      linear_output = np.dot(X, self.weights) + self.bias\n",
        "      y_pred = self.step_function(linear_output)\n",
        "      return y_pred\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "perceptron = Perceptron(learning_rate=0.1, epochs=10)\n",
        "perceptron.fit(X, y)\n",
        "\n",
        "test_data = np.array([[1, 1], [0, 1]])\n",
        "predictions = perceptron.predict(test_data)\n",
        "print(\"AND predictions:\",predictions)\n"
      ],
      "metadata": {
        "id": "Wvn9EClxdg_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68660a48-8daf-4f58-81f9-84c438a4094b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND predictions: [1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Activation function with OR\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 1, 1, 1])\n",
        "\n",
        "perceptron = Perceptron(learning_rate=0.1, epochs=10)\n",
        "perceptron.fit(X, y)\n",
        "\n",
        "test_data = np.array([[1, 1], [0, 1]])\n",
        "predictions = perceptron.predict(test_data)\n",
        "print(\"OR predictions:\",predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRN_KsWseu9j",
        "outputId": "59864d21-2878-440a-d109-9a3c497c2a45"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OR predictions: [1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Activation function with XOR\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 1, 1, 0])\n",
        "\n",
        "perceptron = Perceptron(learning_rate=0.1, epochs=10)\n",
        "perceptron.fit(X, y)\n",
        "\n",
        "test_data = np.array([[1, 1], [0, 1]])\n",
        "predictions = perceptron.predict(test_data)\n",
        "print(\"XOR predictions:\",predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XjX8tXOfiWC",
        "outputId": "4f1e244b-ad1d-4b68-9e4a-389d72aa00bc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR predictions: [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MLP with Single Hidden Layer\n",
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# --- Activation Function ---\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# --- Neuron Class ---\n",
        "class Neuron:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def feedforward(self, inputs):\n",
        "        \"\"\"Compute neuron output using weights, bias, and activation function.\"\"\"\n",
        "        total = np.dot(self.weights, inputs) + self.bias\n",
        "        return sigmoid(total)\n",
        "class OurNeuralNetwork:\n",
        "  '''\n",
        "  A neural network with:\n",
        "    - 2 inputs\n",
        "    - a hidden layer with 2 neurons (h1, h2)\n",
        "    - an output layer with 1 neuron (o1)\n",
        "  Each neuron has the same weights and bias:\n",
        "    - w = [0, 1]\n",
        "    - b = 0\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    weights = np.array([0, 1])\n",
        "    bias = 0\n",
        "\n",
        "  # The Neuron class here is from the previous section\n",
        "    self.h1 = Neuron(weights, bias)\n",
        "    self.h2 = Neuron(weights, bias)\n",
        "    self.o1 = Neuron(weights, bias)\n",
        "\n",
        "  def feedforward(self, x):\n",
        "    out_h1 = self.h1.feedforward(x)\n",
        "    out_h2 = self.h2.feedforward(x)\n",
        "\n",
        "    # The inputs for o1 are the outputs from h1 and h2\n",
        "    out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
        "\n",
        "    return out_o1\n",
        "\n",
        "network = OurNeuralNetwork()\n",
        "x = np.array([2, 3])\n",
        "print(network.feedforward(x))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiaCvjgufwIh",
        "outputId": "4698781e-4a5f-4713-fd5c-d16bb7d72864"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7216325609518421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MNIST dataset\n",
        "# Import necessary libraries\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=0)\n",
        "\n",
        "# Create a Perceptron model\n",
        "perceptron = Perceptron(alpha=0.1)\n",
        "\n",
        "#Train the Perceptron model on the training data\n",
        "perceptron.fit(X_train, y_train)\n",
        "\n",
        "y_pred = perceptron.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model (percentage of correct predictions)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d4RFg1YjgfsS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a6a1f6d-4641-45e6-b656-2c894811f63b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.datasets as skl_data\n",
        "data, labels = skl_data.fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "print(data.shape)\n",
        "data.head()\n",
        "\n",
        "# Normalize the pixel values to range 0-1 for faster and better training\n",
        "data = data / 255.0\n",
        "print(data)\n",
        "\n",
        "print(labels)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=.10, random_state=42, stratify=labels)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "\n",
        "import sklearn.neural_network as skl_nn\n",
        "\n",
        "# Initialize the MLP Classifier\n",
        "mlp = skl_nn.MLPClassifier(hidden_layer_sizes=(1000), max_iter=10, verbose=1, random_state=1,learning_rate_init=0.01)\n",
        "#The network learns the relationship between the pixel features (X_train) and the digit labels (y_train).\n",
        "mlp.fit(X_train,y_train)\n",
        "print(\"Training set score\", mlp.score(X_train, y_train))\n",
        "print(\"Testing set score\", mlp.score(X_test, y_test))\n",
        "\n",
        "\n",
        "index = 346\n",
        "#Extract the digit's features and reshape it for prediction (from 784 features to a 1x784 input matrix).\n",
        "test_digit = X_test.iloc[index].to_numpy().reshape(1,784)\n",
        "test_digit_prediciton = mlp.predict(test_digit)[0]\n",
        "print(\"Predicted value\",test_digit_prediciton)\n",
        "print(\"Actual value\",y_test.iloc[index])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE8MQFmMg6DJ",
        "outputId": "3bb1c656-1a29-426a-cbf8-5d89bf4a5864"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70000, 784)\n",
            "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
            "0         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "1         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "2         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "3         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "4         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
            "69995     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "69996     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "69997     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "69998     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "69999     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "\n",
            "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
            "0          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "1          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "2          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "3          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "4          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "...        ...  ...       ...       ...       ...       ...       ...   \n",
            "69995      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "69996      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "69997      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "69998      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "69999      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "\n",
            "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
            "0           0.0       0.0       0.0       0.0       0.0  \n",
            "1           0.0       0.0       0.0       0.0       0.0  \n",
            "2           0.0       0.0       0.0       0.0       0.0  \n",
            "3           0.0       0.0       0.0       0.0       0.0  \n",
            "4           0.0       0.0       0.0       0.0       0.0  \n",
            "...         ...       ...       ...       ...       ...  \n",
            "69995       0.0       0.0       0.0       0.0       0.0  \n",
            "69996       0.0       0.0       0.0       0.0       0.0  \n",
            "69997       0.0       0.0       0.0       0.0       0.0  \n",
            "69998       0.0       0.0       0.0       0.0       0.0  \n",
            "69999       0.0       0.0       0.0       0.0       0.0  \n",
            "\n",
            "[70000 rows x 784 columns]\n",
            "0        5\n",
            "1        0\n",
            "2        4\n",
            "3        1\n",
            "4        9\n",
            "        ..\n",
            "69995    2\n",
            "69996    3\n",
            "69997    4\n",
            "69998    5\n",
            "69999    6\n",
            "Name: class, Length: 70000, dtype: category\n",
            "Categories (10, object): ['0', '1', '2', '3', ..., '6', '7', '8', '9']\n",
            "(63000, 784)\n",
            "(7000, 784)\n",
            "Iteration 1, loss = 0.23691015\n",
            "Iteration 2, loss = 0.09624868\n",
            "Iteration 3, loss = 0.08107510\n",
            "Iteration 4, loss = 0.07058976\n",
            "Iteration 5, loss = 0.05756001\n",
            "Iteration 6, loss = 0.06131176\n",
            "Iteration 7, loss = 0.05724067\n",
            "Iteration 8, loss = 0.05834494\n",
            "Iteration 9, loss = 0.05683407\n",
            "Iteration 10, loss = 0.05031145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set score 0.9926984126984127\n",
            "Testing set score 0.9781428571428571\n",
            "Predicted value 9\n",
            "Actual value 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}